knitr::opts_chunk$set(echo = TRUE)
library(rpart)
library(rpart.plot)
library(mosaic)
library(ipred)
#bringing Brant's predacc function into the environment
PredAcc = function(y,ypred){
RMSEP = sqrt(mean((y-ypred)^2))
MAE = mean(abs(y-ypred))
MAPE = mean(abs(y-ypred)/y)*100
cat("RMSEP\n")
cat("===============\n")
cat(RMSEP,"\n\n")
cat("MAE\n")
cat("===============\n")
cat(MAE,"\n\n")
cat("MAPE\n")
cat("===============\n")
cat(MAPE,"\n\n")
return(data.frame(RMSEP=RMSEP,MAE=MAE,MAPE=MAPE))
}
Chi.test <- read.csv(file.choose(),header = TRUE)
Chi.train <- read.csv(file.choose(), header = TRUE)
knitr::opts_chunk$set(echo = TRUE)
library(rpart)
library(rpart.plot)
library(mosaic)
library(ipred)
#bringing Brant's predacc function into the environment
PredAcc = function(y,ypred){
RMSEP = sqrt(mean((y-ypred)^2))
MAE = mean(abs(y-ypred))
MAPE = mean(abs(y-ypred)/y)*100
cat("RMSEP\n")
cat("===============\n")
cat(RMSEP,"\n\n")
cat("MAE\n")
cat("===============\n")
cat(MAE,"\n\n")
cat("MAPE\n")
cat("===============\n")
cat(MAPE,"\n\n")
return(data.frame(RMSEP=RMSEP,MAE=MAE,MAPE=MAPE))
}
chi.test <- Chi.test[,-3]
chi.train <- Chi.train[,-3]
fit <- rpart(log(ListPrice)~.,data=ChiTrain,cp=0.068,minsplit=10)
fit <- rpart(log(ListPrice)~.,data=chi.train,cp=0.068,minsplit=10)
home.bag <- bagging(fit,data=chi.train,coob=T,nbagg=1000,control=rpart.control(cp=0.068,minsplit=10,xval=0))
fit <- rpart(log(ListPrice)~.,data=chi.train)
home.bag <- bagging(fit,data=chi.train,coob=T,nbagg=1000,control=rpart.control(cp=0.068,minsplit=10,xval=0))
home.bag <- bagging(formula(fit),data=chi.train,coob=T,nbagg=1000,control=rpart.control(cp=0.068,minsplit=10,xval=0))
ypredlog=predict(home.bag,newdata=chi.test)
ypred = exp(ypredlog)
PredAcc(chi.test$ListPrice,ypred)
?rpart
fit <- rpart(log(ListPrice)~.,data=chi.train)
home.bag <- bagging(formula(fit),data=chi.train,coob=T,nbagg=1000,control=rpart.control(cp=0.0001,minsplit=0,xval=0))
ypredlog=predict(home.bag,newdata=chi.test)
ypred = exp(ypredlog)
PredAcc(chi.test$ListPrice,ypred)
fit <- rpart(log(ListPrice)~.,data=chi.train)
home.bag <- bagging(formula(fit),data=chi.train,coob=T,nbagg=1000,control=rpart.control(cp=0.0001,minsplit=5,xval=0))
ypredlog=predict(home.bag,newdata=chi.test)
ypred = exp(ypredlog)
PredAcc(chi.test$ListPrice,ypred)
fit <- rpart(log(ListPrice)~.,data=chi.train)
home.bag <- bagging(formula(fit),data=chi.train,coob=T,nbagg=1000,control=rpart.control(cp=0.0001,minsplit=2,xval=0))
ypredlog=predict(home.bag,newdata=chi.test)
ypred = exp(ypredlog)
PredAcc(chi.test$ListPrice,ypred)
fit <- rpart(log(ListPrice)~.,data=chi.train)
home.bag <- bagging(formula(fit),data=chi.train,coob=T,nbagg=1000,control=rpart.control(cp=0.00001,minsplit=2,xval=0))
ypredlog=predict(home.bag,newdata=chi.test)
ypred = exp(ypredlog)
PredAcc(chi.test$ListPrice,ypred)
fit <- rpart(log(ListPrice)~.,data=chi.train)
home.bag <- bagging(formula(fit),data=chi.train,coob=T,nbagg=1000,control=rpart.control(cp=0.000001,minsplit=2,xval=0))
ypredlog=predict(home.bag,newdata=chi.test)
ypred = exp(ypredlog)
PredAcc(chi.test$ListPrice,ypred)
fit <- rpart(log(ListPrice)~.,data=chi.train)
home.bag <- bagging(formula(fit),data=chi.train,coob=T,nbagg=1000,control=rpart.control(cp=0.00000001,minsplit=2,xval=0))
ypredlog=predict(home.bag,newdata=chi.test)
ypred = exp(ypredlog)
PredAcc(chi.test$ListPrice,ypred)
fit <- rpart(log(ListPrice)~.,data=chi.train)
home.bag <- bagging(formula(fit),data=chi.train,coob=T,nbagg=1000,control=rpart.control(cp=0.00001,minsplit=2,xval=0))
ypredlog=predict(home.bag,newdata=chi.test)
ypred = exp(ypredlog)
PredAcc(chi.test$ListPrice,ypred)
fit <- rpart(log(ListPrice)~.,data=chi.train)
home.bag <- bagging(formula(fit),data=chi.train,coob=T,nbagg=1000,control=rpart.control(cp=0.00000000001,minsplit=2,xval=0))
ypredlog=predict(home.bag,newdata=chi.test)
ypred = exp(ypredlog)
PredAcc(chi.test$ListPrice,ypred)
fit <- rpart(log(ListPrice)~.,data=chi.train)
home.bag <- bagging(formula(fit),data=chi.train,coob=T,nbagg=1000,control=rpart.control(cp=0.00000001,minsplit=2,xval=0))
ypredlog=predict(home.bag,newdata=chi.test)
ypred = exp(ypredlog)
PredAcc(chi.test$ListPrice,ypred)
solu.train <- read.csv(file.choose(), header = TRUE)
solu.test <- read.csv(file.choose(), header = TRUE)
names(solu.train)
install.packages("randomForest")
library(randomForest)
solu.rf <- randomForest(log10sol~.,data=solu.train)
solu.rf
solu.rf
varImpPlot(solu.rf)
install.packages("plotmo")
library(plotmo)
par(mfrow=c(2,2))
partialPlot(solu.rf,solu.train,MolWeight)
partialPlot(solu.rf,solu.train,NumCarbon)
partialPlot(solu.rf,solu.train,NumNonHBonds)
partialPlot(solu.rf,solu.train,SurfaceArea1)
plotmo(solu.rf)
rf.sscv = function(fit,data,p=.667,B=100,mtry=fit$mtry,ntree=fit$ntree) {
MSE = rep(0,B)
MAE = rep(0,B)
MAPE = rep(0,B)
y = exp(fit$y)
n = nrow(data)
ss <- floor(n*p)
for (i in 1:B) {
sam = sample(1:n,ss,replace=F)
fit2 = randomForest(formula(fit),data=data[sam,],mtry=mtry,ntree=ntree)
ynew = exp(predict(fit2,newdata=data[-sam,]))
MSE[i] = mean((y[-sam]-ynew)^2)
MAE[i] = mean(abs(y[-sam]-ynew))
MAPE[i] = mean((abs(y[-sam]-ynew)/y[-sam]))
}
RMSEP = sqrt(mean(MSE))
MAEP = mean(MAE)
MAPEP = mean(MAPE)
cat("RMSEP\n")
cat("===============\n")
cat(RMSEP,"\n\n")
cat("MAE\n")
cat("===============\n")
cat(MAEP,"\n\n")
cat("MAPE\n")
cat("===============\n")
cat(MAPEP,"\n\n")
temp = data.frame(MSEP=MSE,MAEP=MAE,MAPEP=MAPE)
return(temp)
}
rf.sscv(solu.rf,chi.train)
rf.sscv(solu.rf,solu.train)
home.rf <- randomForest(log(ListPrice)~.,data=chi.train)
home.rf
rf.sscv(home.rf,chi.train,B=25)
rf.sscv(home.rf,chi.train,B=25,mtry=5)
rf.sscv(home.rf,chi.train,B=100,mtry=5,ntree=400)
rf.sscv(home.rf,chi.train,B=100,mtry=6,ntree=400)
plotmo(home.rf)
ypredlog=predict(home.rf,newdata=chi.test)
ypred = exp(ypredlog)
PredAcc(chi.test$ListPrice,ypred)
install.packages("Cubist")
library(Cubist)
Xtrain = solu.train[,-1]
Ytrain = solu.train[,1]
Xtest = solu.test[,-1]
ytest = solu.test[,1]
solu.cub = cubist(Xtrain,Ytrain,committees=10, neighbors =9)
summary(solu.cub)
ypred = predict(solu.cub,newdata=Xtest)
PredAcc(ytest,ypred)
plot(log(solu.train$Price,ypred),xlab="Actual log(Solubility)",ylab="Predict log(Solubility)")
names(Xtrain)
names(solu.train)
plot(log(solu.train$log10sol,ypred),xlab="Actual log(Solubility)",ylab="Predict log(Solubility)")
#Making a plot
plot(solu.train$log10sol,ypred,xlab="Actual log(Solubility)",ylab="Predict log(Solubility)")
plot(solu.test$log10sol,ypred,xlab="Actual log(Solubility)",ylab="Predict log(Solubility)")
abline(0,1,lwd=3,col="blue")
title(main="Fitted Values for Testing Data")
PredAcc(ytest,ypred)
setwd("~/HalbleibWinstatsReu/Learning Exercises/Supervised Learning")
library(rpart)
library(mosaic)
#Brant's function for calculating misclassification rate
misclass = function(fit,y) {
temp <- table(fit,y)
cat("Table of Misclassification\n")
cat("(row = predicted, col = actual)\n")
print(temp)
cat("\n\n")
numcor <- sum(diag(temp))
numinc <- length(y) - numcor
mcr <- numinc/length(y)
cat(paste("Misclassification Rate = ",format(mcr,digits=3)))
cat("\n")
}
knitr::opts_chunk$set(echo = TRUE)
library(rpart)
library(mosaic)
#Package in R for future use: XGBOOST. Constructs boosted trees. Not in today's example but has many options for tweaking and improving performance.
#Brant's function for building a misclassification table and printing the missclassification rate
misclass = function(fit,y) {
temp <- table(fit,y)
cat("Table of Misclassification\n")
cat("(row = predicted, col = actual)\n")
print(temp)
cat("\n\n")
numcor <- sum(diag(temp))
numinc <- length(y) - numcor
mcr <- numinc/length(y)
cat(paste("Misclassification Rate = ",format(mcr,digits=3)))
cat("\n")
}
BreastDiag <- read.csv(file.choose(), header =TRUE)
View(BreastDiag)
str(BreastDiag)
?cubist
samp <- sample(1:569, floor(569*.666667), replace = FALSE)
BCtrain <- BreastDiag[samp,]
BCtest <- BreastDiag[-samp,]
bc.rpart <- rpart(Diagnosis~., data = BCtrain)
plot(bc.rpart)
text(bc.rpart)
post(bc.rpart)
plot(bc.rpart)
text(bc.rpart)
```{r, fig.height=9,fig.width=9}
#Loading data from the BreastDiag.csv file
#BreastDiag <- read.csv(file.choose(), header =TRUE)
#Making a train and a test set (in the real world, we'd want to split 3x: train, test, and validation sets)
samp <- sample(1:569, floor(569*.666667), replace = FALSE)
BCtrain <- BreastDiag[samp,]
BCtest <- BreastDiag[-samp,]
bc.rpart <- rpart(Diagnosis~., data = BCtrain)
plot(bc.rpart)
text(bc.rpart)
yfit <- predict(bc.rpart,type="class")
misclass(yfit, BCtrain$Diagnosis)
library(rpart.plot)
prp(bc.rpart)
summary(bc.rpart)
bc.rpart2 <- rpart(Diagnosis~.,data=BCtrain,cp=0.0001,minsplit=4)
bc.rpart2 <- rpart(Diagnosis~.,data=BCtrain,cp=0.0001,minsplit=4)
yfit <- predict(bc.rpart2,type="class")
misclass(yfit,BCtrain$Diagnosis)
prp(bc.rpart2, type=4,extra=3,cex=0.7)
ypred = predict(bc.rpart,newdata=BCtest,type="class")
misclass(ypred,BCtest$Diagnosis)
ypred = predict(bc.rpart,newdata=BCtest,type="class")
misclass(ypred,BCtest$Diagnosis)
#Model 2
ypred = predict(bc.rpart2,newdata=BCtest,type="class")
misclass(ypred,BCtest$Diagnosis)
bc.bag <- bagging(Diagnosis~.,data=BCtrain, nbagg=100,control=rpart.control(cp=0.019,minsplit=5,xval=0),coob=TRUE)
bc.bag
knitr::opts_chunk$set(echo = TRUE)
library(rpart) #basic tree library
library(rpart.plot)
library(ipred) #bagging library
library(mosaic)
#Package in R for future use: XGBOOST. Constructs boosted trees. Not in today's example but has many options for tweaking and improving performance.
#Brant's function for building a misclassification table and printing the missclassification rate
misclass = function(fit,y) {
temp <- table(fit,y)
cat("Table of Misclassification\n")
cat("(row = predicted, col = actual)\n")
print(temp)
cat("\n\n")
numcor <- sum(diag(temp))
numinc <- length(y) - numcor
mcr <- numinc/length(y)
cat(paste("Misclassification Rate = ",format(mcr,digits=3)))
cat("\n")
}
#Using bc.bag to predict
ypred <- predict(bc.bag,newdata=BCtest, type = "class")
misclass(ypred,BCtest$Diagnosis)
library(randomForest)
knitr::opts_chunk$set(echo = TRUE)
library(rpart) #basic tree library
library(rpart.plot)
library(ipred) #bagging library
library(randomForest) #random foreset library
library(mosaic)
#Package in R for future use: XGBOOST. Constructs boosted trees. Not in today's example but has many options for tweaking and improving performance.
#Brant's function for building a misclassification table and printing the missclassification rate
misclass = function(fit,y) {
temp <- table(fit,y)
cat("Table of Misclassification\n")
cat("(row = predicted, col = actual)\n")
print(temp)
cat("\n\n")
numcor <- sum(diag(temp))
numinc <- length(y) - numcor
mcr <- numinc/length(y)
cat(paste("Misclassification Rate = ",format(mcr,digits=3)))
cat("\n")
}
bc.rf <- randomForest(Diagnosis~.data=BCtrain)
bc.rf <- randomForest(Diagnosis~.,data=BCtrain)
bc.rf
varImpPlot(bc.rf)
#using bc.rf to predict
ypred <- predict(bc.rf,newdata = BCtest,type="class")
misclass(ypred,BCtest$Diagnosis)
install.packages("ada")
knitr::opts_chunk$set(echo = TRUE)
library(rpart) #basic tree library (can look up the vignette to review all the underlying math in this library... cool!)
library(rpart.plot)
library(ipred) #bagging library
library(randomForest) #random foreset library
library(ada) #boosting for classification library
#can also use xgboost (most complicated and difficult to train, not used here but can use in cases where ada proves insufficient), also have a library called adabag that might be of interest
#May also want to look into caret package (has an accompanying book called Predictive Modeling), very robust and pulls in a lot of the tree models for you along with extra functions that do useful things with trees
library(mosaic)
#Package in R for future use: XGBOOST. Constructs boosted trees. Not in today's example but has many options for tweaking and improving performance.
#Brant's function for building a misclassification table and printing the missclassification rate
misclass = function(fit,y) {
temp <- table(fit,y)
cat("Table of Misclassification\n")
cat("(row = predicted, col = actual)\n")
print(temp)
cat("\n\n")
numcor <- sum(diag(temp))
numinc <- length(y) - numcor
mcr <- numinc/length(y)
cat(paste("Misclassification Rate = ",format(mcr,digits=3)))
cat("\n")
}
bc.boost <- ada(Diagnosis~.,data=BCtrain)
bc.boost
summary(bc.boost)
#checking the misclassification rate
ypred <- predict(bc.boost,newdata=BCtest)
misclass(ypred,BCtest$Diagnosis)
bc.boost <- ada(Diagnosis~.,data=BCtrain)
bc.boost
summary(bc.boost)
#checking the misclassification rate
ypred <- predict(bc.boost,newdata=BCtest)
misclass(ypred,BCtest$Diagnosis)
bc.boost <- ada(Diagnosis~.,data=BCtrain)
bc.boost
summary(bc.boost)
#checking the misclassification rate
ypred <- predict(bc.boost,newdata=BCtest)
misclass(ypred,BCtest$Diagnosis)
bc.boost <- ada(Diagnosis~.,data=BCtrain)
bc.boost
summary(bc.boost)
#checking the misclassification rate
ypred <- predict(bc.boost,newdata=BCtest)
misclass(ypred,BCtest$Diagnosis)
set.seed(123)
bc.boost <- ada(Diagnosis~.,data=BCtrain)
bc.boost
summary(bc.boost)
#checking the misclassification rate
ypred <- predict(bc.boost,newdata=BCtest)
misclass(ypred,BCtest$Diagnosis)
set.seed(123)
bc.boost <- ada(Diagnosis~.,data=BCtrain)
bc.boost
summary(bc.boost)
#checking the misclassification rate
ypred <- predict(bc.boost,newdata=BCtest)
misclass(ypred,BCtest$Diagnosis)
oils <- read.csv(file.choose(),header = TRUE)
train <- sample(1:572, floor(.666667*572),replace = FALSE)
oil.train <- oils[train,]
oil.test <- oils[-train,]
oil.rpart <- rpart(Area.name~.,data-oil.train)
?rpart
oil.rpart <- rpart(Area.name~.,data=oil.train)
oil.rpart
ypred <- predict(oil.rpart,newdata=oil.train)
misclass(ypred,oil.train$Area.name)
#Loading data from OliveOils.csv
oils <- read.csv(file.choose(),header = TRUE)
#Making a training and a testing set
train <- sample(1:572, floor(.666667*572),replace = FALSE)
oil.train <- oils[train,]
oil.test <- oils[-train,]
#fitting a single tree
oil.rpart <- rpart(Area.name~.,data=oil.train)
#Checking misclassification
ypred <- predict(oil.rpart,newdata=oil.test)
misclass(ypred,oil.test$Area.name)
#Loading data from OliveOils.csv
oils <- read.csv(file.choose(),header = TRUE)
#Making a training and a testing set
train <- sample(1:572, floor(.666667*572),replace = FALSE)
oil.train <- oils[train,]
oil.test <- oils[-train,]
#fitting a single tree
oil.rpart <- rpart(Area.name~.,data=oil.train)
#Checking misclassification
ypred <- predict(oil.rpart,newdata=oil.test)
misclass(ypred,oil.test$Area.name)
knitr::opts_chunk$set(echo = TRUE)
library(rpart) #basic tree library (can look up the vignette to review all the underlying math in this library... cool!)
library(rpart.plot)
library(ipred) #bagging library
library(randomForest) #random foreset library
library(ada) #boosting for classification library
#can also use xgboost (most complicated and difficult to train, not used here but can use in cases where ada proves insufficient), also have a library called adabag that might be of interest
#May also want to look into caret package (has an accompanying book called Predictive Modeling), very robust and pulls in a lot of the tree models for you along with extra functions that do useful things with trees
library(mosaic)
#Package in R for future use: XGBOOST. Constructs boosted trees. Not in today's example but has many options for tweaking and improving performance.
#Brant's function for building a misclassification table and printing the missclassification rate
misclass = function(fit,y) {
temp <- table(fit,y)
cat("Table of Misclassification\n")
cat("(row = predicted, col = actual)\n")
print(temp)
cat("\n\n")
numcor <- sum(diag(temp))
numinc <- length(y) - numcor
mcr <- numinc/length(y)
cat(paste("Misclassification Rate = ",format(mcr,digits=3)))
cat("\n")
}
misclass(ypred,oil.test$Area.name)
knitr::opts_chunk$set(echo = TRUE)
library(rpart) #basic tree library (can look up the vignette to review all the underlying math in this library... cool!)
library(rpart.plot)
library(ipred) #bagging library
library(randomForest) #random foreset library
library(ada) #boosting for classification library
#can also use xgboost (most complicated and difficult to train, not used here but can use in cases where ada proves insufficient), also have a library called adabag that might be of interest
#May also want to look into caret package (has an accompanying book called Predictive Modeling), very robust and pulls in a lot of the tree models for you along with extra functions that do useful things with trees
library(mosaic)
#Package in R for future use: XGBOOST. Constructs boosted trees. Not in today's example but has many options for tweaking and improving performance.
#Brant's function for building a misclassification table and printing the missclassification rate
misclass = function(fit,y) {
temp <- table(fit,y)
cat("Table of Misclassification\n")
cat("(row = predicted, col = actual)\n")
print(temp)
cat("\n\n")
numcor <- sum(diag(temp))
numinc <- length(y) - numcor
mcr <- numinc/length(y)
cat(paste("Misclassification Rate = ",format(mcr,digits=3)))
cat("\n")
}
#Loading data from OliveOils.csv
oils <- read.csv(file.choose(),header = TRUE)
#Making a training and a testing set
train <- sample(1:572, floor(.666667*572),replace = FALSE)
oil.train <- oils[train,]
oil.test <- oils[-train,]
#fitting a single tree
oil.rpart <- rpart(Area.name~.,data=oil.train)
#Checking misclassification
ypred <- predict(oil.rpart,newdata=oil.test)
misclass(ypred,oil.test$Area.name)
ypred2 <- predict(oil.rpart,newdata=oil.test)
misclass(ypred2,oil.test$Area.name)
